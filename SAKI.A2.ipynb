{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SAKI.A2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"lNJmaMsAZ57m","colab_type":"text"},"source":["Install Spacy and Flair, if it is not installed yet. Load the needed libraries and change path to the work folder in google drive"]},{"cell_type":"code","metadata":{"id":"5dIRKhDQZ5SG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1445},"outputId":"047f10bd-2551-4f96-b622-2888e869c127","executionInfo":{"status":"ok","timestamp":1560883718815,"user_tz":-120,"elapsed":19950,"user":{"displayName":"Dominik Nitschmann","photoUrl":"https://lh6.googleusercontent.com/-TY3KHPpQAf4/AAAAAAAAAAI/AAAAAAAAA5E/D_s_SZX_kzA/s64/photo.jpg","userId":"15218744798729618703"}}},"source":["! pip install -U spacy\n","! pip install flair"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.1.4)\n","Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n","Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n","Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.4)\n","Requirement already satisfied, skipping upgrade: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.4)\n","Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.2)\n","Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n","Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n","Requirement already satisfied, skipping upgrade: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n","Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.0.6)\n","Requirement already satisfied, skipping upgrade: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.6.0)\n","Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.3.9)\n","Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n","Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy) (4.28.1)\n","Collecting flair\n","  Using cached https://files.pythonhosted.org/packages/4e/3a/2e777f65a71c1eaa259df44c44e39d7071ba8c7780a1564316a38bf86449/flair-0.4.2-py3-none-any.whl\n","Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n","Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.0.3)\n","Requirement already satisfied: pytest>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n","Collecting pytorch-pretrained-bert>=0.6.1 (from flair)\n","  Using cached https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl\n","Collecting segtok>=1.5.7 (from flair)\n","  Using cached https://files.pythonhosted.org/packages/1d/59/6ed78856ab99d2da04084b59e7da797972baa0efecb71546b16d48e49d9b/segtok-1.5.7.tar.gz\n","Collecting regex (from flair)\n","  Using cached https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.3)\n","Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n","Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.1.0)\n","Collecting mpld3==0.3 (from flair)\n","Collecting bpemb>=0.2.9 (from flair)\n","  Using cached https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair) (0.0)\n","Collecting sqlitedict>=1.6.0 (from flair)\n","  Using cached https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n","Collecting deprecated>=1.2.4 (from flair)\n","  Using cached https://files.pythonhosted.org/packages/9f/7a/003fa432f1e45625626549726c2fbb7a29baa764e9d1fdb2323a5d779f8a/Deprecated-1.2.5-py2.py3-none-any.whl\n","Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.3.0)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.16.4)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.12.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.8.4)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.5.3)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.8.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (0.7.1)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (19.1.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (41.0.1)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (7.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.1->flair) (2.21.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.1->flair) (1.9.167)\n","Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.8.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.3)\n","Collecting sentencepiece (from bpemb>=0.2.9->flair)\n","  Using cached https://files.pythonhosted.org/packages/00/95/7f357995d5eb1131aa2092096dca14a6fc1b1d2860bd99c22a612e1d1019/sentencepiece-0.1.82-cp36-cp36m-manylinux1_x86_64.whl\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair) (0.21.2)\n","Requirement already satisfied: wrapt<2,>=1 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.11.1)\n","Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (2019.3.9)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.2.1)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.9.4)\n","Requirement already satisfied: botocore<1.13.0,>=1.12.167 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (1.12.167)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->flair) (0.13.2)\n","Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.167->boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.14)\n","Building wheels for collected packages: segtok, regex, sqlitedict\n","  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/15/ee/a8/6112173f1386d33eebedb3f73429cfa41a4c3084556bcee254\n","  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n","  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n","Successfully built segtok regex sqlitedict\n","Installing collected packages: regex, pytorch-pretrained-bert, segtok, mpld3, sentencepiece, bpemb, sqlitedict, deprecated, flair\n","Successfully installed bpemb-0.3.0 deprecated-1.2.5 flair-0.4.2 mpld3-0.3 pytorch-pretrained-bert-0.6.2 regex-2019.6.8 segtok-1.5.7 sentencepiece-0.1.82 sqlitedict-1.6.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ywY_wwZQchOM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"946f4930-2edb-474c-d34f-c0ceff7da206","executionInfo":{"status":"ok","timestamp":1560883774345,"user_tz":-120,"elapsed":75458,"user":{"displayName":"Dominik Nitschmann","photoUrl":"https://lh6.googleusercontent.com/-TY3KHPpQAf4/AAAAAAAAAAI/AAAAAAAAA5E/D_s_SZX_kzA/s64/photo.jpg","userId":"15218744798729618703"}}},"source":["import json\n","from google.colab import drive\n","import os\n","import spacy\n","from spacy.gold import biluo_tags_from_offsets\n","import pandas as pd\n","import re\n","from flair.data import Corpus\n","from flair.datasets import ColumnCorpus\n","from flair.trainers import ModelTrainer\n","from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n","from flair.visual.training_curves import Plotter\n","from flair.models import SequenceTagger\n","\n","drive.mount('/content/gdrive')\n","root = \"/content/gdrive/My Drive/FAU/SAKI.A2\"\n","os.chdir(root) \n","\n","from spacy_train_resume_ner import train_spacy_ner"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uOaEj-COZxpM","colab_type":"text"},"source":["# Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"qTXSfWgn2kmI","colab_type":"text"},"source":["Load the dataset, save the content in all_resumes."]},{"cell_type":"code","metadata":{"id":"yo7_YC3HW0JB","colab_type":"code","colab":{}},"source":["dataset_path = \"data/Entity Recognition in Resumes.json\"\n","with open(dataset_path,encoding=\"utf8\") as f:\n","    lines = f.readlines()\n","    \n","all_resumes = []\n","\n","for line in lines:\n","    all_resumes.append(json.loads(line))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zwfmciIqX7i4","colab_type":"text"},"source":["Convert data to spacy format and filter out resumes that have no entities. Data is preprocessed now for usage in spacy."]},{"cell_type":"code","metadata":{"id":"L-awl3EEW0Li","colab_type":"code","colab":{}},"source":["def convert_data(data):\n","    text = data['content']\n","    entities = []\n","    if data['annotation'] is not None:\n","        for annotation in data['annotation']:\n","            point = annotation['points'][0]\n","            labels = annotation['label']\n","            if not isinstance(labels, list):\n","                labels = [labels]\n","            for label in labels:\n","                entities.append((point['start'], point['end'] + 1, label))\n","    return (text, {\"entities\": entities})\n","   \n","converted_resumes = [convert_data(res) for res in all_resumes]\n","converted_resumes = [res for res in converted_resumes if len(res[1][\"entities\"]) > 0]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SSZNs-7FahRm","colab_type":"text"},"source":["We have the data in spacy format now. We use flair, but spacy has a nice helper function to convert it to the BILOU data format which is recognizable for flair - instead of rewriting it, we use spacy just for this functionality. \n","\n","We choose the entity labels 'College', 'Degree' and 'Skills' and filter the dataset."]},{"cell_type":"code","metadata":{"id":"gciNS_nqW0Ux","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"cc1cf4d1-de50-4246-88df-b54337f36032","executionInfo":{"status":"ok","timestamp":1560883776237,"user_tz":-120,"elapsed":77298,"user":{"displayName":"Dominik Nitschmann","photoUrl":"https://lh6.googleusercontent.com/-TY3KHPpQAf4/AAAAAAAAAAI/AAAAAAAAA5E/D_s_SZX_kzA/s64/photo.jpg","userId":"15218744798729618703"}}},"source":["chosen_entity_labels = [\"College Name\", \"Degree\", \"Skills\"]\n","\n","def gather_candidates(dataset,entity_labels):\n","    candidates = list()\n","    for resume in dataset:\n","        res_ent_labels = list(zip(*resume[1][\"entities\"]))[2]\n","        if set(entity_labels).issubset(res_ent_labels):\n","            candidates.append(resume)\n","    return candidates\n","\n","training_data = gather_candidates(converted_resumes, chosen_entity_labels)\n","print(\"Gathered {} training examples\".format(len(training_data)))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Gathered 380 training examples\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fvrBDtu4bOV_","colab_type":"text"},"source":["Now that we have our training data, we remove all but the chosen entity annotations from this data, so that the model we train will only train for our entities. X is our training_data then."]},{"cell_type":"code","metadata":{"id":"w72d9vArW0Xc","colab_type":"code","colab":{}},"source":["ents = []\n","for resume in training_data:\n","    ents.append(resume[1][\"entities\"])\n","\n","X = []\n","i = 0\n","for resume in training_data:\n","    X.append([])\n","    for ents in resume[1][\"entities\"]:\n","        if ents[2] in chosen_entity_labels:\n","            X[i].append(ents)\n","    i += 1\n","\n","i = 0\n","for resume in training_data:\n","    resume[1][\"entities\"] = []\n","    resume[1][\"entities\"].extend(X[i])\n","    i += 1\n","    \n","X = training_data"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YMc6vYYM3CVu","colab_type":"text"},"source":["Sometimes, there is bad data in it which is not working with spaCy. We filter these out."]},{"cell_type":"code","metadata":{"id":"xjwElqJwW0aE","colab_type":"code","colab":{}},"source":["def remove_bad_data(training_data):\n","    return training_data\n","    model, baddocs = train_spacy_ner(training_data, debug=True, n_iter=1)\n","    filtered = [data for data in training_data if data[0] not in baddocs]\n","    print(\"Unfiltered training data size: \",len(training_data))\n","    print(\"Filtered training data size: \", len(filtered))\n","    print(\"Bad data size: \", len(baddocs))\n","    return filtered\n","\n","X = remove_bad_data(X)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"njtpaggB3IdT","colab_type":"text"},"source":["Now, we split the dataset into train and test datasets. We use 80% training data and 20% test data."]},{"cell_type":"code","metadata":{"id":"TBs2h5Q8W0cb","colab_type":"code","colab":{}},"source":["def train_test_split(X,train_percent):\n","    train_size = int((len(X) / 100) * train_percent)\n","    train = X[0:train_size]\n","    test = X[train_size:]\n","    return train,test\n","  \n","train, test = train_test_split(X, 80)\n","assert (len(train) + len(test)) == len(X)    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PBNoDLZcef46","colab_type":"text"},"source":["We can now train the spacy model. Wait, train? Didn't we want flair to train and test? Yes, that's true, but in order to use the BILOU-export helper function, we have to train at least one epoch with spacy, we just need one model. That's okay for this routine, so let's do this."]},{"cell_type":"code","metadata":{"id":"M527tP6AW0en","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"3ca388bc-c62b-44a4-b554-631fc8b5f441","executionInfo":{"status":"ok","timestamp":1560883800093,"user_tz":-120,"elapsed":101109,"user":{"displayName":"Dominik Nitschmann","photoUrl":"https://lh6.googleusercontent.com/-TY3KHPpQAf4/AAAAAAAAAAI/AAAAAAAAA5E/D_s_SZX_kzA/s64/photo.jpg","userId":"15218744798729618703"}}},"source":["nlp,_ = train_spacy_ner(train,n_iter=1)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Created blank 'en' model\n","Losses {'ner': 40290.960953430884}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Fb7QnATT3Rgw","colab_type":"text"},"source":["Now that we trained the model, we can generate the bilou format with panda dataframes."]},{"cell_type":"code","metadata":{"id":"gpQrFxP2W0ks","colab_type":"code","colab":{}},"source":["def make_bilou_df(nlp,resume):\n","    doc = nlp(resume[0])\n","    bilou_ents_predicted = biluo_tags_from_offsets(doc, [(ent.start_char,ent.end_char,ent.label_)for ent in doc.ents])\n","    bilou_ents_true = biluo_tags_from_offsets(doc, [(ent[0], ent[1], ent[2]) for ent in resume[1][\"entities\"]])\n","\n","    doc_tokens = [tok.text for tok in doc]\n","    bilou_df = pd.DataFrame()\n","    bilou_df[\"Tokens\"] = doc_tokens\n","    bilou_df[\"Tokens\"] = bilou_df[\"Tokens\"].str.replace(\"\\\\s+\",\"\") \n","    bilou_df[\"Predicted\"] = bilou_ents_predicted\n","    bilou_df[\"True\"] = bilou_ents_true\n","    return bilou_df\n","\n","training_data_as_bilou = [make_bilou_df(nlp,res) for res in train]\n","test_data_as_bilou = [make_bilou_df(nlp,res) for res in test]\n","\n","training_df = pd.DataFrame(columns = [\"text\",\"ner\",\"doc\",\"ner_spacy\"])\n","test_df = pd.DataFrame(columns = [\"text\",\"ner\",\"doc\",\"ner_spacy\"])\n","for idx,df in enumerate(training_data_as_bilou):\n","    df2 = pd.DataFrame()\n","    df2[\"text\"] = df[\"Tokens\"]\n","    df2[\"ner\"] = df[\"True\"]\n","    df2[\"ner_spacy\"]=df[\"Predicted\"]\n","    df2[\"doc\"]=idx\n","    training_df = training_df.append(df2, sort=True)\n","for idx,df in enumerate(test_data_as_bilou):\n","    df2 = pd.DataFrame()\n","    df2[\"text\"] = df[\"Tokens\"]\n","    df2[\"ner\"] = df[\"True\"]\n","    df2[\"ner_spacy\"]=df[\"Predicted\"]\n","    df2[\"doc\"]=idx\n","    test_df = test_df.append(df2, sort=True)\n","\n","with open(\"data/flair/train_res_bilou.txt\",'w+',encoding=\"utf-8\") as f:\n","    training_df.to_csv(f,sep=\" \",encoding=\"utf-8\",index=False)\n","with open(\"data/flair/test_res_bilou.txt\",'w+',encoding=\"utf-8\") as f:\n","    test_df.to_csv(f,sep=\" \",encoding=\"utf-8\",index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NEwiPovRgmJT","colab_type":"text"},"source":["**Manipulating the BILOU**\n","\n","We have several problems with the generated BILOU file. First, flair takes sentences which are seperated by an empty line. We have no empty line which means we have one big sentence - we have to split it. Second, we look for skills, which are seperated by different characters but are annotated as one big skill. E. g. this is recognized as one skill, when it should be three skills.\n","1) SAP 2) Photoshop 3) Office \n","\n","We try to manipulate the BILOU dataset that it fits the needs. That means we have to restructure it. For example, we have three lines:\n","\n","\n","\n","```\n","69 B-Skills O 1\n","69 I-Skills O )\n","69 I-Skills O SAP\n","69 I-Skills O 2\n","69 I-Skills O )\n","69 I-Skills O Photoshop\n","69 I-Skills O )\n","69 I-Skills O 3\n","69 L-Skills O Office\n","```\n","\n","We want to convert it to this so it looks for SAP, Photoshop and Office as units.\n","\n","\n","```\n","69 O O 1\n","69 O O )\n","69 U-Skills O SAP\n","69 O O 2\n","69 O O )\n","69 U-Skills O Photoshop\n","69 O O )\n","69 O O 3\n","69 U-Skills O Office\n","```\n","\n","Very similar, it should also keep in mind there are skills which are no units. For example:\n","\n","```\n","69 B-Skills O 1\n","69 I-Skills O )\n","69 I-Skills O SAP\n","69 I-Skills O ABAP\n","69 I-Skills O 2\n","69 I-Skills O )\n","69 I-Skills O Adobe\n","69 I-Skills O Photoshop\n","69 I-Skills O CS5\n","69 I-Skills O )\n","69 I-Skills O 3\n","69 I-Skills O Microsoft\n","69 L-Skills O Office\n","```\n","\n","After conversion\n","\n","```\n","69 O O 1\n","69 O O )\n","69 B-Skills O SAP\n","69 L-Skills O ABAP\n","69 O O 2\n","69 O O )\n","69 B-Skills O Adobe\n","69 I-Skills O Photoshop\n","69 L-Skills O CS5\n","69 O O )\n","69 O O 3\n","69 B-Skills O Microsoft\n","69 L-Skills O Office\n","```"]},{"cell_type":"code","metadata":{"id":"MsjYGehsW0nJ","colab_type":"code","colab":{}},"source":["# Taken from https://stackoverflow.com/a/1884277\n","def find_nth(haystack, needle, n):\n","    start = haystack.find(needle)\n","    while start >= 0 and n > 1:\n","        start = haystack.find(needle, start+len(needle))\n","        n -= 1\n","    return start\n","    \n","def get_number_of_training_sample(line):\n","    return line[0:find_nth(line, ' ', 1)]\n","  \n","def get_token_of_training_sample(line):\n","    return line[find_nth(line, ' ', 3)+1:]\n","  \n","def get_label_of_training_sample(line):\n","    if line.find(\" O O \") != -1:\n","        return None\n","    first_space = find_nth(line, ' ', 1)\n","    second_space = find_nth(line, ' ', 2)\n","    return line[first_space+3:second_space]\n","  \n","def get_bilou_type_of_training_sample(line):\n","    first_space = find_nth(line, ' ', 1)\n","    return line[first_space + 1:first_space + 2]\n","  \n","def replace_bilou_type(line, new_bilou_character):\n","    first_dash = line.find('-')\n","    return line[0:first_dash - 1] + new_bilou_character + \"-\" + line[first_dash + 1:]\n","\n","def fix_bilou_format(content):\n","    new_content = []\n","    lineNr = 0\n","    previous_training_sample = 0\n","    for line in content:\n","        if lineNr == 0:\n","            lineNr = lineNr + 1\n","            continue\n","        current_training_sample = get_number_of_training_sample(line)\n","\n","        token = get_token_of_training_sample(line)\n","        label = get_label_of_training_sample(line)\n","        bilou_type = get_bilou_type_of_training_sample(line)\n","\n","        if bilou_type == \"U\":\n","            new_content.append(line)\n","            lineNr = lineNr + 1\n","            continue\n","\n","        token_has_characters = re.search(\"[a-zA-Z]\", token) \n","        if lineNr == 1:\n","            previous_bilou_type = \"O\"\n","        else:\n","            previous_bilou_type = get_bilou_type_of_training_sample(new_content[lineNr - 2])\n","\n","        if token_has_characters is None:\n","            # if token has no character and the previous one is a B token,\n","            # we have to set it to a U token\n","            # if token has no character and the previous one is a I token,\n","            # we have to set it to a L token\n","            # also, since it has no characters, we say its an O token \n","            if previous_bilou_type == \"B\":\n","                new_content[lineNr - 2] = new_content[lineNr - 2].replace(\"B-\", \"U-\")\n","            elif previous_bilou_type == \"I\":\n","                new_content[lineNr - 2] = new_content[lineNr - 2].replace(\"I-\", \"L-\")\n","            new_content.append(line.replace(bilou_type, \"O\"))\n","        else:\n","            # if token has characters, the bilou type is not O and the previous one is a O, U or L token,\n","            # we have to set it to a B token\n","            # if token has characters, the bilou type is not O and the previous one is a B or I token,\n","            # we have to set it to a I token\n","\n","            if bilou_type == \"O\":\n","                new_content.append(line)\n","            else:\n","                if previous_bilou_type == \"O\" or previous_bilou_type == \"U\" or previous_bilou_type == \"L\":\n","                    new_string = replace_bilou_type(line, \"B\")\n","                    new_content.append(new_string)\n","                elif previous_bilou_type == \"B\" or previous_bilou_type == \"I\":\n","                    new_string = replace_bilou_type(line, \"I\")\n","                    new_content.append(new_string)\n","                else:\n","                    new_content.append(line)\n","\n","        lineNr = lineNr + 1\n","    pre_last_bilou_token = get_bilou_type_of_training_sample(new_content[len(new_content)-2])\n","    last_bilou_token = get_bilou_type_of_training_sample(new_content[len(new_content)-1])\n","    \n","    if last_bilou_token != \"O\":\n","        if pre_last_bilou_token == \"B\" or pre_last_bilou_token == \"I\":\n","            new_content[len(new_content)-1] = new_content[len(new_content)-1].replace(\"B-\", \"L-\")\n","            new_content[len(new_content)-1] = new_content[len(new_content)-1].replace(\"I-\", \"L-\")\n","        else:\n","            new_content[len(new_content)-1] = new_content[len(new_content)-1].replace(\"I-\", \"U-\")\n","            new_content[len(new_content)-1] = new_content[len(new_content)-1].replace(\"B-\", \"U-\")\n","    return new_content\n","  \n","def make_sentences(content, regex_expressions):\n","    new_content = []\n","    for line in content:\n","        empty_line_appended = False\n","        token = get_token_of_training_sample(line)\n","        for regex_expression in regex_expressions:\n","            if re.search(regex_expression, token) is not None:\n","                new_content.append(line)\n","                new_content.append('\\n')\n","                empty_line_appended = True\n","                break\n","        if empty_line_appended == True:\n","            continue\n","        new_content.append(line)            \n","    return new_content"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZMkxsGKVW0p6","colab_type":"code","colab":{}},"source":["with open(\"data/flair/train_res_bilou.txt\") as f:\n","    train_content = f.readlines()\n","    \n","with open(\"data/flair/test_res_bilou.txt\") as f:\n","    test_content = f.readlines()\n","\n","fixed_bilou_train = fix_bilou_format(train_content)\n","fixed_bilou_test = fix_bilou_format(test_content)\n","\n","# we skip the header, thats why we add 1\n","assert len(fixed_bilou_train) + 1 == len(train_content)\n","assert len(fixed_bilou_test) + 1 == len(test_content)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rGpfFEFq5ufm","colab_type":"text"},"source":["The data is now (almost) preprocessed and fixed. The following code illustrates how the result looks."]},{"cell_type":"code","metadata":{"id":"2gAdZnJmW0tQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"outputId":"f3790009-1c28-4015-feb1-94df29f0e4a6","executionInfo":{"status":"ok","timestamp":1560883820211,"user_tz":-120,"elapsed":121176,"user":{"displayName":"Dominik Nitschmann","photoUrl":"https://lh6.googleusercontent.com/-TY3KHPpQAf4/AAAAAAAAAAI/AAAAAAAAA5E/D_s_SZX_kzA/s64/photo.jpg","userId":"15218744798729618703"}}},"source":["test_content_bilou = [\n","    'doc ner ner_spacy text',\n","    '69 B-Skills O 1',\n","    '69 I-Skills O )',\n","    '69 I-Skills O SAP',\n","    '69 I-Skills O ABAP',\n","    '69 I-Skills O 2',\n","    '69 I-Skills O )',\n","    '69 I-Skills O Adobe',\n","    '69 I-Skills O Photoshop',\n","    '69 I-Skills O CS5',\n","    '69 I-Skills O )',\n","    '69 I-Skills O 3',\n","    '69 I-Skills O Microsoft',\n","    '69 L-Skills O Office',\n","]\n","\n","test_content_bilou_u_token = [\n","    'doc ner ner_spacy text',\n","    '69 B-Skills O 1',\n","    '69 I-Skills O )',\n","    '69 I-Skills O SAP',\n","    '69 I-Skills O 2',\n","    '69 I-Skills O )',\n","    '69 I-Skills O Adobe',\n","    '69 I-Skills O )',\n","    '69 I-Skills O 3',\n","    '69 L-Skills O Microsoft',\n","]\n","\n","test_content_bilou_fixed = fix_bilou_format(test_content_bilou)\n","test_content_bilou_u_token_fixed = fix_bilou_format(test_content_bilou_u_token)\n","for t in test_content_bilou_fixed:\n","    print(t)\n","    \n","print(\"\\n\")\n","for t in test_content_bilou_u_token_fixed:\n","    print(t)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["69 O-Skills O 1\n","69 O-Skills O )\n","69 B-Skills O SAP\n","69 L-Skills O ABAP\n","69 O-Skills O 2\n","69 O-Skills O )\n","69 B-Skills O Adobe\n","69 I-Skills O Photoshop\n","69 L-Skills O CS5\n","69 O-Skills O )\n","69 O-Skills O 3\n","69 B-Skills O Microsoft\n","69 L-Skills O Office\n","\n","\n","69 O-Skills O 1\n","69 O-Skills O )\n","69 U-Skills O SAP\n","69 O-Skills O 2\n","69 O-Skills O )\n","69 U-Skills O Adobe\n","69 O-Skills O )\n","69 O-Skills O 3\n","69 U-Skills O Microsoft\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cFhZTAeq3PkB","colab_type":"text"},"source":["Now, we want to make sentences out of our data. We have the make_sentences function which takes the content and an array of regular expressions. If one of the regular expression matches the lines, we add an empty line to create a sentence. Having said that, the array of regular expressions are like delimiters and search for specific occurences of characters and when they occur, this is a sign for creating a new sentence."]},{"cell_type":"code","metadata":{"id":"Xl73z7QAW0wN","colab_type":"code","colab":{}},"source":["# First regular expression searches for a point\n","regular_expressions = [\"^\\.$\"]\n","ready_train_set = make_sentences(fixed_bilou_train, regular_expressions)\n","ready_test_set = make_sentences(fixed_bilou_test, regular_expressions)\n","\n","with open('/content/gdrive/My Drive/FAU/SAKI.A2/data/flair/train_res_bilou_preprocessed.txt', 'w') as f:\n","    for item in ready_train_set:\n","        f.write(\"%s\" % item)\n","        \n","with open('/content/gdrive/My Drive/FAU/SAKI.A2/data/flair/test_res_bilou_preprocessed.txt', 'w') as f:\n","    for item in ready_test_set:\n","        f.write(\"%s\" % item)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HkKvBlnsz5Mp","colab_type":"text"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"9Hdzf9h26Bf2","colab_type":"text"},"source":["The data is finally preprocessed and we can use it to train our Flair Model. First, we read the data into a corpus and specify the relevant columns."]},{"cell_type":"code","metadata":{"id":"4EuL0irHW00t","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"d80c0751-3f21-485f-a154-40048f693f7a","executionInfo":{"status":"ok","timestamp":1560883825316,"user_tz":-120,"elapsed":126225,"user":{"displayName":"Dominik Nitschmann","photoUrl":"https://lh6.googleusercontent.com/-TY3KHPpQAf4/AAAAAAAAAAI/AAAAAAAAA5E/D_s_SZX_kzA/s64/photo.jpg","userId":"15218744798729618703"}}},"source":["columns = {1: 'ner', 3: 'text' }\n","tag_type = 'ner'\n","data_folder = '/content/gdrive/My Drive/FAU/SAKI.A2/data/flair'\n","\n","corpus: Corpus = ColumnCorpus(data_folder, columns,\n","                                     train_file='train_res_bilou_preprocessed.txt',\n","                                     test_file='test_res_bilou_preprocessed.txt',\n","                                     dev_file=None)\n","\n","print(len(corpus.train))\n","print(len(corpus.dev))\n","print(len(corpus.test))\n","  \n","tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n","print(tag_dictionary.idx2item)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["2019-06-18 18:50:20,955 Reading data from /content/gdrive/My Drive/FAU/SAKI.A2/data/flair\n","2019-06-18 18:50:20,957 Train: /content/gdrive/My Drive/FAU/SAKI.A2/data/flair/train_res_bilou_preprocessed.txt\n","2019-06-18 18:50:20,959 Dev: None\n","2019-06-18 18:50:20,961 Test: /content/gdrive/My Drive/FAU/SAKI.A2/data/flair/test_res_bilou_preprocessed.txt\n","6268\n","696\n","2027\n","[b'<unk>', b'O', b'O-Skills', b'B-Skills', b'L-Skills', b'U-Degree', b'O-Degree', b'U-Skills', b'B-Degree', b'I-Degree', b'L-Degree', b'\"B-College', b'\"L-College', b'I-Skills', b'\"I-College', b'-', b'<START>', b'<STOP>']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vBZZCsbi6aKA","colab_type":"text"},"source":["Now we can specify the Embeddings which we want to use. Embeddings can be combined with the StackedEmbeddings class. Flair recommends the usage of the glove embeddings with their pre-trained embeddings and this is what worked best for this dataset, so we use it."]},{"cell_type":"code","metadata":{"id":"bkflgXeYW05X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":343},"outputId":"6b866e9e-2218-4563-85aa-3e8337a46933","executionInfo":{"status":"ok","timestamp":1560883856815,"user_tz":-120,"elapsed":157703,"user":{"displayName":"Dominik Nitschmann","photoUrl":"https://lh6.googleusercontent.com/-TY3KHPpQAf4/AAAAAAAAAAI/AAAAAAAAA5E/D_s_SZX_kzA/s64/photo.jpg","userId":"15218744798729618703"}}},"source":["stacked_embeddings = StackedEmbeddings([\n","                                        WordEmbeddings('glove'), \n","                                        FlairEmbeddings('news-forward'), \n","                                        FlairEmbeddings('news-backward')\n","                                       ])\n","\n","tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n","                                        embeddings=stacked_embeddings,\n","                                        tag_dictionary=tag_dictionary,\n","                                        tag_type=tag_type,\n","                                        use_crf=True)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["2019-06-18 18:50:25,477 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmpanau6egz\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 160000128/160000128 [00:07<00:00, 21044552.90B/s]"],"name":"stderr"},{"output_type":"stream","text":["2019-06-18 18:50:33,684 copying /tmp/tmpanau6egz to cache at /root/.flair/embeddings/glove.gensim.vectors.npy\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["2019-06-18 18:50:33,925 removing temp file /tmp/tmpanau6egz\n","2019-06-18 18:50:34,480 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim not found in cache, downloading to /tmp/tmpd6pabjj3\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 21494764/21494764 [00:01<00:00, 13535940.18B/s]"],"name":"stderr"},{"output_type":"stream","text":["2019-06-18 18:50:36,511 copying /tmp/tmpd6pabjj3 to cache at /root/.flair/embeddings/glove.gensim\n","2019-06-18 18:50:36,534 removing temp file /tmp/tmpd6pabjj3\n"],"name":"stdout"},{"output_type":"stream","text":["\n","/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"stream","text":["2019-06-18 18:50:38,506 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4.1/big-news-forward--h2048-l1-d0.05-lr30-0.25-20/news-forward-0.4.1.pt not found in cache, downloading to /tmp/tmpr3i07_iu\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 73034624/73034624 [00:03<00:00, 18805839.82B/s]"],"name":"stderr"},{"output_type":"stream","text":["2019-06-18 18:50:42,865 copying /tmp/tmpr3i07_iu to cache at /root/.flair/embeddings/news-forward-0.4.1.pt\n","2019-06-18 18:50:42,935 removing temp file /tmp/tmpr3i07_iu\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["2019-06-18 18:50:51,134 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4.1/big-news-backward--h2048-l1-d0.05-lr30-0.25-20/news-backward-0.4.1.pt not found in cache, downloading to /tmp/tmp6jhj4i6o\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 73034575/73034575 [00:03<00:00, 20587480.29B/s]"],"name":"stderr"},{"output_type":"stream","text":["2019-06-18 18:50:55,153 copying /tmp/tmp6jhj4i6o to cache at /root/.flair/embeddings/news-backward-0.4.1.pt\n","2019-06-18 18:50:55,234 removing temp file /tmp/tmp6jhj4i6o\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"n3e7WRPs6oJG","colab_type":"text"},"source":["Lets train the model now and print out the results!"]},{"cell_type":"code","metadata":{"id":"2KzgwGheW07q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"9762cf9c-1bcf-48c0-c12d-4f81d13768be"},"source":["trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n","\n","trainer.train('resources/taggers/resume-ner-40',\n","              learning_rate=0.1,\n","              mini_batch_size=32,\n","              max_epochs=40)\n","\n","plotter = Plotter()\n","plotter.plot_training_curves('resources/taggers/resume-ner-40/loss.tsv')\n","plotter.plot_weights('resources/taggers/resume-ner-40/weights.txt')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2019-06-18 18:50:56,660 ----------------------------------------------------------------------------------------------------\n","2019-06-18 18:50:56,665 Evaluation method: MICRO_F1_SCORE\n","2019-06-18 18:50:57,460 ----------------------------------------------------------------------------------------------------\n","2019-06-18 18:50:59,298 epoch 1 - iter 0/196 - loss 92.63227844\n","2019-06-18 18:51:38,020 epoch 1 - iter 19/196 - loss 24.16082734\n","2019-06-18 18:52:04,713 epoch 1 - iter 38/196 - loss 17.20774236\n","2019-06-18 18:52:33,124 epoch 1 - iter 57/196 - loss 14.76257630\n","2019-06-18 18:53:07,591 epoch 1 - iter 76/196 - loss 12.56564394\n","2019-06-18 18:53:40,299 epoch 1 - iter 95/196 - loss 11.11060275\n","2019-06-18 18:54:10,788 epoch 1 - iter 114/196 - loss 10.36144746\n"],"name":"stdout"}]}]}